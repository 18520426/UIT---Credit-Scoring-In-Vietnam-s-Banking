{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"kalapa_paper_cb.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"ycq1MeJnMms1"},"source":["# 1. Library & Input data"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"3nmtU3OXMms3"},"source":["import pandas as pd\n","import numpy as np\n","\n","from datetime import datetime\n","from unidecode import unidecode\n","from itertools import combinations\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.metrics import roc_auc_score\n","\n","from scipy import interp\n","from sklearn.metrics import auc\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import plot_roc_curve\n","\n","from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","\n","import category_encoders as ce\n","\n","import re\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","sns.set(color_codes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"PlvYkKBVMms8"},"source":["train = pd.read_csv('/kaggle/input/klps-creditscring-challenge-for-students/train.csv')\n","test = pd.read_csv('/kaggle/input/klps-creditscring-challenge-for-students/test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A51hbkMMMmtC"},"source":["# 2. Feature Engineering"]},{"cell_type":"code","metadata":{"trusted":true,"id":"yJsDtrCyMmtD"},"source":["# Drop some columns are duplicated, with correlation = NaN\n","ignore_columns = ([\"gioiTinh\", \"info_social_sex\", \"ngaySinh\", \"namSinh\"] + \n","        [f\"Field_{c}\" for c in [14, 16, 17, 24, 26, 30, 31, 37, 52, 57]] + \n","        ['partner0_K', 'partner0_L', \n","         'partner1_B', 'partner1_D', 'partner1_E', 'partner1_F', 'partner1_K', 'partner1_L',\n","         'partner2_B', 'partner2_G', 'partner2_K', 'partner2_L',\n","         'partner3_B', 'partner3_C', 'partner3_F', 'partner3_G', 'partner3_H', 'partner3_K', 'partner3_L',\n","         *['partner4_' + i for i in 'ABCDEFGHK'],\n","         'partner5_B', 'partner5_C', 'partner5_H', 'partner5_K', 'partner5_L'])\n","\n","# Some auto columns could make new better columns\n","all_auto_columns = list(set([c for c in train.columns if train[c].dtype in [np.int64, np.float64]])\n","                    .difference(ignore_columns + ['currentLocationLocationId', 'homeTownLocationId', 'label', 'id']))\n","\n","auto_columns_1 = [c for c in all_auto_columns if 'Field_' in c]\n","auto_columns_2 = [c for c in all_auto_columns if 'partner' in c]\n","auto_columns_3 = [c for c in all_auto_columns if 'num' in c]\n","auto_columns_4 = [c for c in all_auto_columns if c not in auto_columns_1 + auto_columns_2 + auto_columns_3]\n","print(len(auto_columns_1), len(auto_columns_2), len(auto_columns_3), len(auto_columns_4), len(all_auto_columns))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZUJ4LxELMmtI"},"source":["### 2.1. Datetime columns"]},{"cell_type":"code","metadata":{"trusted":true,"id":"mD2q55BcMmtI"},"source":["date_cols = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 35, 40]]\n","datetime_cols = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\n","correct_dt_cols = ['Field_34', 'ngaySinh']\n","cat_cols = date_cols + datetime_cols + correct_dt_cols\n","\n","# Normalize Field_34, ngaySinh\n","def ngaysinh_34_normalize(s):\n","    if s != s: return np.nan\n","    try: s = int(s)\n","    except ValueError: s = s.split(\" \")[0]\n","    return datetime.strptime(str(s)[:6], \"%Y%m\")\n","\n","# Normalize datetime data\n","def datetime_normalize(s):\n","    if s != s: return np.nan\n","    s = s.split(\".\")[0]\n","    if s[-1] == \"Z\": s = s[:-1]\n","    return datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n","\n","# Normalize date data\n","def date_normalize(s):\n","    if s != s: return np.nan\n","    try: t = datetime.strptime(s, \"%m/%d/%Y\")\n","    except: t = datetime.strptime(s, \"%Y-%m-%d\")\n","    return t\n","\n","def process_datetime_cols(df):\n","    df[datetime_cols] = df[datetime_cols].applymap(datetime_normalize)  \n","    df[date_cols] = df[date_cols].applymap(date_normalize)\n","    df[correct_dt_cols] = df[correct_dt_cols].applymap(ngaysinh_34_normalize)\n","\n","    # Some delta columns\n","    for i, j in zip('43 1 2'.split(), '1 2 44'.split()): df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.seconds\n","    for i, j in zip('5 6 7 33 8 11 9 15 25 6 7 8 9 15 25 2'.split(), '6 34 33 40 11 35 15 25 32 7 8 9 15 25 32 8'.split()): \n","        df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.days\n","    \n","    # Age, month\n","    df['age'] = 2020 - pd.DatetimeIndex(df['ngaySinh']).year\n","    df['birth_month'] = pd.DatetimeIndex(df['ngaySinh']).month\n","    \n","    # Days from current time & isWeekday\n","    for col in cat_cols:\n","        name = col.split('_')[-1]\n","        df[f'is_WD_{name}'] = df[col].dt.dayofweek.isin(range(5))\n","        df[f'days_from_now_{name}'] = (datetime.now() - pd.DatetimeIndex(df[col])).days\n","        df[col] = df[col].dt.strftime('%m-%Y')\n","    \n","    # Delta for x_startDate and x_endDate\n","    for cat in ['F', 'E', 'C', 'G', 'A']:\n","        df[f'{cat}_startDate'] = pd.to_datetime(df[f\"{cat}_startDate\"], infer_datetime_format=True)\n","        df[f'{cat}_endDate'] = pd.to_datetime(df[f\"{cat}_endDate\"], infer_datetime_format=True)\n","        \n","        df[f'{cat}_start_end'] = (df[f'{cat}_endDate'] - df[f'{cat}_startDate']).dt.days\n","        \n","    for i, j in zip('F E C G'.split(), 'E C G A'.split()):\n","        df[f'{j}_{i}_startDate'] = (df[f'{j}_startDate'] - df[f'{i}_startDate']).dt.days\n","        df[f'{j}_{i}_endDate'] = (df[f'{j}_endDate'] - df[f'{i}_endDate']).dt.days\n","    \n","    temp_date = [f'{i}_startDate' for i in 'ACEFG'] + [f'{i}_endDate' for i in 'ACEFG']\n","    \n","    for col in temp_date:\n","        df[col] = df[col].dt.strftime('%m-%Y')\n","        \n","    for col in cat_cols + temp_date:\n","        df[col] = df[col]\n","        \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AaMmKXGoMmtM"},"source":["### 2.2. Categorical columns"]},{"cell_type":"code","metadata":{"trusted":true,"id":"LqVG-t2PMmtM"},"source":["unicode_cols = ['Field_18', 'maCv', 'diaChi', 'Field_46', 'Field_48', 'Field_49', 'Field_56', 'Field_61', 'homeTownCity', \n","                'homeTownName', 'currentLocationCity', 'currentLocationName', 'currentLocationState', 'homeTownState']\n","object_cols = (unicode_cols + \n","               [f'Field_{str(i)}' for i in '4 12 36 38 47 62 45 54 55 65 66 68'.split()] +\n","               ['data.basic_info.locale', 'currentLocationCountry', 'homeTownCountry', 'brief'])\n","\n","def str_normalize(s):\n","    s = str(s).strip().lower()\n","    s = re.sub(' +', \" \", s)\n","    return s\n","\n","def combine_gender(s):\n","    x, y = s \n","    if x != x and y != y: return \"nan\"\n","    if x != x: return y.lower()\n","    return x.lower()\n","\n","def process_categorical_cols(df):\n","    df['diaChi'] = df['diaChi'].str.split(',').str[-1]\n","    df[unicode_cols] = df[unicode_cols].applymap(str_normalize).applymap(lambda x: unidecode(x) if x==x else x)\n","    \n","    # Normalize some columns\n","    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n","    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngoài quốc doanh Quận 7\": np.nan})\n","    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n","    \n","    # Make some new features\n","    df['Field_45_Q'] = df['Field_45'].str[:-3].astype('category')\n","    df['Field_45_TP_55'] = df['Field_45'].str[:2] == df['Field_55']\n","    df['is_homeTown_diaChi'] = df['homeTownCity'] == df['diaChi']\n","    df['is_homeTown_current_city'] = df['homeTownCity'] == df['currentLocationCity']\n","    df['is_homeTown_current_state'] = df['homeTownState'] == df['currentLocationState']\n","    df['F48_49'] = df['Field_48'] == df['Field_49']\n","    \n","    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n","    \n","    df[[\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n","        \"homeTownLatitude\", \"homeTownLongitude\"]].replace(0, np.nan, inplace=True) # value == 0: noisy\n","\n","    df[[\"currentLocationLocationId\", \"homeTownLocationId\"]] = (df[[\"currentLocationLocationId\", \"homeTownLocationId\"]]\n","                                                             .applymap(str_normalize).astype(\"category\"))\n","    df[object_cols] = df[object_cols].astype('category')\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xyJbTkZNMmtP"},"source":["### 2.3. Others"]},{"cell_type":"code","metadata":{"trusted":true,"id":"qvHyPgrYMmtQ"},"source":["# New feature from columns 63, 64\n","def process_63_64(z):\n","    x, y = z\n","    if x != x and y != y:\n","        return np.nan\n","    if (x, y) in [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 8.0), (7.0, 5.0), (5.0, 6.0), (9.0, 43.0), (8.0, 9.0)]: return True\n","    else: return False\n","    \n","def process_others(df):        \n","    df[[\"Field_27\", \"Field_28\"]].replace(0.0, np.nan, inplace=True)\n","    df['F18_isnumeric'] = df['Field_18'].str.isnumeric()\n","    df['F18_isalpha'] = df['Field_18'].str.isalpha()\n","    \n","    # Delta from some pairs of columns\n","    for i, j in [(20, 27), (28, 27), (39, 41), (41, 42), (50, 51), (51, 53)]:\n","        df[f'F{str(i)}_{str(j)}_delta'] = df[f'Field_{str(j)}'] - df[f'Field_{str(i)}']\n","    df['F_59_60'] = df['Field_59'] - df['Field_60'] - 2\n","    df['F_63_64'] = df[['Field_63', 'Field_64']].apply(process_63_64, axis=1).astype('category')\n","    \n","    # Mean, std from partnerX columns\n","    for i in '1 2 3 4 5'.split():\n","        col = [c for c in df.columns if f'partner{i}' in c]\n","        df[f'partner{i}_mean'] = df[col].mean(axis=1)\n","        df[f'partner{i}_std'] = df[col].std(axis=1)\n","\n","    # Reference columns\n","    columns = set(df.columns).difference(ignore_columns)\n","    df['cnt_NaN'] = df[columns].isna().sum(axis=1)\n","    df['cnt_True'] = df[columns].applymap(lambda x: isinstance(x, bool) and x).sum(axis=1)\n","    df['cnt_False'] = df[columns].applymap(lambda x: isinstance(x, bool) and not x).sum(axis=1)\n","\n","    # Combinations of auto columns\n","    lst_combination = (list(combinations(auto_columns_2, 2)) + list(combinations(auto_columns_3, 2)) + list(combinations(auto_columns_4, 2)))\n","    for l, r in lst_combination:\n","        for func in 'add subtract divide multiply'.split():\n","            df[f'auto_{func}_{l}_{r}'] = getattr(np, func)(df[l], df[r])\n","            \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3yGnJxDcMmtW"},"source":["### 2.4. Combine all parts"]},{"cell_type":"code","metadata":{"trusted":true,"id":"hSOFAQ2NMmtX"},"source":["def transform(df):\n","    df = process_datetime_cols(df)\n","    df = process_categorical_cols(df)\n","    df = process_others(df)\n","    return df.drop(ignore_columns, axis=1)\n","\n","train = transform(train)\n","test = transform(test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myUVOXvEMmtc"},"source":["## Datetime preprocessing - Add some columns"]},{"cell_type":"code","metadata":{"trusted":true,"id":"RT2p2sktMmtc"},"source":["def split_dates(df):\n","    dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n","    for date in dates:\n","        df[date+'_day'] = df[date].dt.day\n","        df[date+'_month'] = df[date].dt.month\n","        df[date+'_year'] = df[date].dt.year\n","        df[date+'_week'] = df[date].dt.week\n","        df[date+'_dayofweek'] = df[date].dt.dayofweek\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"mmIrY1XLMmtf"},"source":["def days_between_startEnd(df):\n","    start_dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']]\n","    end_dates = [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n","    col = ['F','E','C','G','A']\n","    for i in range(5):\n","        df[col[i]+'_delta'] = (df[end_dates[i]]-df[start_dates[i]]).dt.total_seconds()/(60*60*24)\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"uQcAtA9gMmti"},"source":["def to_datetime(df):\n","    dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n","    for col in dates:\n","        df[col] = pd.to_datetime(df[col], errors='coerce')\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"oZjLlvhkMmtm"},"source":["dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n","dates_columns = ['F_delta','E_delta','C_delta','G_delta','A_delta']\n","for d in dates:\n","    dates_columns.append(d+'_day')\n","    dates_columns.append(d+'_month')\n","    dates_columns.append(d+'_year')\n","    dates_columns.append(d+'_week')\n","    dates_columns.append(d+'_dayofweek')\n","dates_columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"RQT7q5bmMmtq"},"source":["def impute_df(df):    \n","    for col in dates_columns:\n","        df[col] = df[col].fillna(df[col].mean())\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"bujKkiaXMmtt"},"source":["train = to_datetime(train)\n","test = to_datetime(test)\n","train = split_dates(train)\n","test = split_dates(test)\n","train = days_between_startEnd(train)\n","test = days_between_startEnd(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"fi3LDJ4bMmtw"},"source":["train = impute_df(train)\n","test = impute_df(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Y5N8kAt2Mmt6"},"source":["train.to_csv(\"final_train.csv\", index=0)\n","test.to_csv(\"final_test.csv\", index=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"jtrC3nTlMmt9"},"source":["# train = pd.read_csv('./final_train.csv')\n","# test = pd.read_csv('./final_test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Kd9ZNyqMmt_"},"source":["### 2.5. Try Count Encoding"]},{"cell_type":"code","metadata":{"trusted":true,"id":"RG-UNiBCMmuA"},"source":["#Support catboost modelling\n","#take categorical columns\n","cat_features = [c for c in train.columns if (train[c].dtype not in [np.float64, np.int64])]\n","train[cat_features] = train[cat_features].astype(str)\n","test[cat_features] = test[cat_features].astype(str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"hO9T5A5nMmuE"},"source":["# Create the encoder\n","t = pd.concat([train, test]).reset_index(drop=True)\n","count_enc = ce.CountEncoder().fit_transform(t[cat_features])\n","tt = t.join(count_enc.add_suffix(\"_count\"))\n","\n","f2_train = tt.loc[tt.index < train.shape[0]]\n","f2_test = tt.loc[tt.index >= train.shape[0]]\n","\n","columns = sorted(set(f2_train.columns).intersection(f2_test.columns))\n","print(len(columns))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g8ZXc_IiMmuI"},"source":["# 3. Modelling"]},{"cell_type":"code","metadata":{"trusted":true,"id":"uQBaTQkRMmuI"},"source":["\n","TRAIN, TEST = f2_train[columns].drop(['id', 'label'], axis=1), f2_test[columns].drop(['id', 'label'], axis=1)\n","LABEL = f2_train['label']\n","preds, test_preds, gini = np.zeros(TRAIN.shape[0]), {}, {}\n","feature_importance_df = pd.DataFrame()\n","\n","###\n","tprs = []\n","aucs = []\n","mean_fpr = np.linspace(0, 1, 100)\n","\n","fold_aucs = []\n","fig, ax = plt.subplots()\n","\n","###\n","\n","\n","cv = StratifiedKFold(n_splits=5, shuffle=True)\n","for i, (train_idx, val_idx) in enumerate(cv.split(TRAIN, LABEL)):\n","    X_train, y_train = TRAIN.iloc[train_idx], LABEL.iloc[train_idx]\n","    X_val, y_val = TRAIN.iloc[val_idx], LABEL.iloc[val_idx]\n","\n","#     model = CatBoostClassifier(eval_metric='AUC', \n","#                              use_best_model=True,\n","#                              iterations=1000, \n","#                              learning_rate=0.1, \n","#                              random_seed=42).fit(X_train, y_train, \n","#                                                  cat_features=set(cat_features),\n","#                                                  eval_set=(X_val, y_val), verbose=500)\n","\n","###\n","    model = CatBoostClassifier(eval_metric='AUC', \n","#                              use_best_model=True,\n","                             iterations=1000, \n","                             learning_rate=0.03).fit(X_train, y_train, \n","                                                  cat_features=set(cat_features))\n","###\n","\n","    y_pred = model.predict(X_val)\n","    y_pred_proba = model.predict_proba(X_val)[:, 1]\n","        \n","    preds[val_idx] = y_pred_proba\n","    test_preds[f'F{i+1}'] = model.predict_proba(TEST)[:, 1]\n","    \n","    gini[f'F{i+1}'] = 2 * roc_auc_score(y_val, y_pred_proba) - 1\n","    \n","###\n","    \n","    auc_ = roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n","    fold_aucs.append(auc_)\n","    print(f\"val AUC = {auc_:.4f}\")\n","\n","\n","    viz = plot_roc_curve(model, X_val, y_val, name=f\"ROC Fold {i}\", alpha=0.3, lw=1, ax=ax)\n","    \n","    interp_tpr = interp(mean_fpr, viz.fpr, viz.tpr)\n","    interp_tpr[0] = 0.0\n","    tprs.append(interp_tpr)\n","    aucs.append(viz.roc_auc)\n","    \n","    # For create feature importances\n","    fold_importance_df = pd.DataFrame()\n","    fold_importance_df[\"feature\"] = TRAIN.columns\n","    fold_importance_df[\"importance\"] = model.feature_importances_\n","    fold_importance_df[\"fold\"] = i + 1\n","    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n","    print('Fold %2d GINI : %.5f' % (i + 1, 2*roc_auc_score(y_val, y_pred_proba) - 1))\n","    \n","ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Random\", alpha=0.8)\n","\n","mean_tpr = np.mean(tprs, axis=0)\n","mean_tpr[-1] = 1.0\n","mean_auc = auc(mean_fpr, mean_tpr)\n","std_auc = np.std(aucs)\n","\n","# fig1, ax1 = plt.subplots()\n","# ax1.plot(\n","#     mean_fpr,\n","#     mean_tpr,\n","#     color=\"b\",\n","#     label=r\"Mean ROC (AUC = %0.3f $\\pm$ %0.3f)\" % (mean_auc, std_auc),\n","#     lw=2,\n","#     alpha=0.8,\n","# )\n","# ax1.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"ROC Curves\", ylabel='True Possitive Rate', xlabel='False Possitive Rate')\n","# ax1.legend(loc=\"lower right\")\n","# plt.savefig(\"roc_mean.png\")\n","\n","ax.plot(\n","    mean_fpr,\n","    mean_tpr,\n","    color=\"b\",\n","    label=r\"Mean ROC (AUC = %0.3f $\\pm$ %0.3f)\" % (mean_auc, std_auc),\n","    lw=2,\n","    alpha=0.8,\n",")\n","\n","std_tpr = np.std(tprs, axis=0)\n","tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n","tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n","ax.fill_between(\n","    mean_fpr,\n","    tprs_lower,\n","    tprs_upper,\n","    color=\"grey\",\n","    alpha=0.2,\n","    label=r\"$\\pm$ 1 std. dev.\",\n",")\n","\n","ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"CatBoost ROC Curve\")\n","ax.legend(loc=\"lower right\")\n","plt.savefig(\"roc.png\")\n","###\n","    \n","# Resulting\n","roc_auc = roc_auc_score(LABEL, preds)\n","print('Avg GINI score:', 2*roc_auc - 1)\n","\n","result = np.array(list(gini.values()))\n","print('GINI: {:.5f} +- {:.5f}'.format(result.mean(), result.std()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"f_jy943_MmuL"},"source":["import joblib\n","\n","\n","joblib.dump(model,'model.pkl')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"jw_rcA3FMmuN"},"source":["loaded_model = joblib.load('./model.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"IqmZUEMHMmuP"},"source":["from sklearn.metrics import classification_report\n","\n","y_pred = loaded_model.predict(TRAIN)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"-B9BLlTJMmuS"},"source":["print(classification_report(LABEL, y_pred, labels=[0, 1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"yyc5ru4BMmuX"},"source":["with open('classification_report.txt', 'w') as f:\n","    for i in classification_report(LABEL, y_pred, labels=[0, 1]):\n","        f.write(str(i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"CN5AufyjMmub"},"source":["def display_importances(feature_importance_df_):\n","    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n","        by=\"importance\", ascending=False)[:30].index\n","    \n","    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n","    \n","    plt.figure(figsize=(12, 8))\n","    sns.barplot(x=\"importance\", y=\"feature\", \n","                data=best_features.sort_values(by=\"importance\", ascending=False))\n","    plt.title('CatBoost Features (avg over folds)')\n","    plt.tight_layout()\n","    plt.savefig('importance.png')\n","\n","display_importances(feature_importance_df_=feature_importance_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kH325WEkMmue"},"source":["# 4. Submisison"]},{"cell_type":"code","metadata":{"trusted":true,"id":"DRO3OK2ZMmuf"},"source":["test['label'] = pd.DataFrame(test_preds).mean(axis=1).values\n","test[['id', 'label']].to_csv('submission.csv', index=False)"],"execution_count":null,"outputs":[]}]}